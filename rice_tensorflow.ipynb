{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPUs will likely run quickly with dtype policy mixed_float16 as they all have compute capability of at least 7.0\n",
      "2.13.0\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.filterwarnings('ignore', category = ConvergenceWarning)\n",
    "\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "from sklearn.metrics import classification_report, f1_score, confusion_matrix\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, Model\n",
    "from keras.layers import Input, Dense, Dropout, BatchNormalization, Activation\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers.experimental import preprocessing\n",
    "from keras.callbacks import Callback, EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import mixed_precision\n",
    "mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './Rice_Image_Dataset'\n",
    "data = {\"imgpath\": [], 'label':[]}\n",
    "category = os.listdir(path)\n",
    "category.pop(3)\n",
    "for folder in category:\n",
    "    folderpath = os.path.join(path, folder)\n",
    "    filelist = os.listdir(folderpath)\n",
    "    for file in filelist:\n",
    "        fpath = os.path.join(folderpath, file)\n",
    "        data['imgpath'].append(fpath)\n",
    "        data['labels'].append(folder)\n",
    "        \n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Convert labels to numbers\n",
    "lb = LabelEncoder()\n",
    "df['encoded_labels'] = lb.fit_transform(df['labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the Dataset into Train & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, Temp_df = train_test_split(df, train_size = 0.9, shuffle = True, random_state = 124)\n",
    "valid_df, test_df = train_test_split(Temp_df, train_size = 0.85, shuffle = True, random_state = 124)\n",
    "train_df = train_df.reset_index(drop = True)\n",
    "valid_df = valid_df.reset_index(drop = True)\n",
    "test_df = test_df.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Image Count Per Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train  = train_df[\"labels\"].value_counts()\n",
    "label = train.tolist()\n",
    "index = train.index.tolist()\n",
    "\n",
    "colors = [\n",
    "    \"#1f77b4\", \"#ff7f0e\", \"#2ca02c\", \"#d62728\", \"#9467bd\",\n",
    "   ]\n",
    "\n",
    "plt.figure(figsize=(17,17))\n",
    "plt.title(\"Training data images count per class\",fontsize=38)\n",
    "plt.xlabel('Number of images', fontsize=35)\n",
    "plt.ylabel('Classes', fontsize=35)\n",
    "plt.barh(index,label, color=colors)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.sample(n=15, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df[[\"imgpath\", \"labels\"]].head(5))\n",
    "print(train_df.shape)\n",
    "print(valid_df[[\"imgpath\", \"labels\"]].head(5))\n",
    "print(valid_df.shape)\n",
    "print(test_df[[\"imgpath\", \"labels\"]].head(5))\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show sample from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,12))\n",
    "for i, row in valid_df.sample(n=16).reset_index().iterrows():\n",
    "    plt.subplot(4,4,i+1)\n",
    "    image_path = row['imgpath']\n",
    "    image = Image.open(image_path)\n",
    "    plt.imshow(image)\n",
    "    plt.title(row[\"labels\"])\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%% time\n",
    "BATCH_SIZE = 25\n",
    "IMAGE_SIZE = (224,224)\n",
    "\n",
    "generator = ImageDataGenerator(preprocessing_function = tf.keras.applications.efficientnet.preprocess_input)\n",
    "\n",
    "#Split the data inot three categories\n",
    "train_images = generator.flow_from_dataframe(\n",
    "    dataframe = train_df,\n",
    "    x_col = 'imgpath'\n",
    "    y_col = 'labels',\n",
    "    target_size = IMAGE_SIZE,\n",
    "    color_mode = 'rgb',\n",
    "    class_mode = 'categorical',\n",
    "    bathc_size = BATCH_SIZE,\n",
    "    shuffle = True,\n",
    "    seed = 42\n",
    ")\n",
    "\n",
    "val_images = generator.flow_from_dataframe(\n",
    "    dataframe = valid_df,\n",
    "    x_col = 'imgpath'\n",
    "    y_col = 'labels',\n",
    "    target_size = IMAGE_SIZE,\n",
    "    color_mode = 'rgb',\n",
    "    class_mode = 'categorical',\n",
    "    bathc_size = BATCH_SIZE,\n",
    "    shuffle = True,\n",
    "    seed = 42\n",
    ")\n",
    "\n",
    "test_images = generator.flow_from_dataframe(\n",
    "    dataframe = test_df,\n",
    "    x_col = 'imgpath'\n",
    "    y_col = 'labels',\n",
    "    target_size = IMAGE_SIZE,\n",
    "    color_mode = 'rgb',\n",
    "    class_mode = 'categorical',\n",
    "    bathc_size = BATCH_SIZE,\n",
    "    shuffle = True,\n",
    "    seed = 42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 미리 학습된 efficientnet 모델을 tensorflow에서 불러옵니다.\n",
    "pretrained_model = tf.keras.applications.EfficientNetB0(\n",
    "    input_shape = (224,224,3),\n",
    "    includde_top = False,   # 맨 마지막 층은 우리가 학습시킬 모델이기 때문에 따로 불러오지 않습니다.\n",
    "    weights = 'imagenet',\n",
    "    pooling = 'max'\n",
    ")\n",
    "\n",
    "# 각 pretrained된 인공신경망의 각 층이 학습되지 않도록 freezing합니다.\n",
    "for i, layer in enumerate(pretrained_model.layers):\n",
    "    pretrained_model.layers[i].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(set(train_images.classes))\n",
    "\n",
    "# Data Augmentation Step\n",
    "augment = keras.Sequential([\n",
    "    keras.layers.RandomFlip(\"horizontal\"),\n",
    "    keras.layers.RandomRotation(0.1),\n",
    "    keras.layers.RandomZoom(0.1),\n",
    "    keras.layers.RandomContrast(0.1)\n",
    "], name = 'AugmentationLayer')\n",
    "\n",
    "inputs = Input(shape = (224,224,3), name = 'inputLayer')\n",
    "x = augment(inputs)\n",
    "pretrain_out = pretrained_model(x, training = False)\n",
    "x = Dense(256)(pretrained_out)\n",
    "x = Activation(activation = 'relu')(x)\n",
    "x = BatchNormlization()(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(num_classes)(x)\n",
    "outputs = Activation(activation = 'softmax', dtype = tf.float32, name = 'activationLayer')(x)\n",
    "\n",
    "model = Model(inputs = inputs, outputs = outputs)\n",
    "\n",
    "model.compile(optimizer = Adam(5e-5),\n",
    "              loss = 'categorical_crossentropy',\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training : Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_images, \n",
    "                    step_per_epoch = len(train_images),\n",
    "                    validation_data = val_images,\n",
    "                    epochs = 50,\n",
    "                    callbacks = [EarlyStopping(monitor = 'val_loss',\n",
    "                                               patience = 10,\n",
    "                                               restore_best_weights = True),\n",
    "                                 ReduceLROnPlateau(monitor = 'val_loss',\n",
    "                                                   factor = 0.2,\n",
    "                                                   patience = 5,\n",
    "                                                   mode = 'min')\n",
    "                                 ]\n",
    "                    )\n",
    "model.save('./my_checkpoints.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeGraph(hist):\n",
    "    train_acc = hist.history['accuracy']\n",
    "    train_loss = hist.history['loss']\n",
    "    val_acc = hist.history['val_accuracy']\n",
    "    val_loss = hist.history['val_loss']\n",
    "    index_loss = np.argmin(val_loss)\n",
    "    val_lowest = val_loss[index_loss]\n",
    "    index_acc = np.argmax(val_acc)\n",
    "    acc_highest = val_acc[index_acc]\n",
    "    Epochs = [i + 1 for i in range(len(train_acc))]\n",
    "    loss_label = f'best epoch : {str(index_loss + 1)}'\n",
    "    acc_label = f'best epoch : {str(index_acc + 1)}'\n",
    "    \n",
    "    # Plot training history\n",
    "    plt.figure(figsize = (20,8))\n",
    "    plt.style.use('fivethirtyeight')\n",
    "    \n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(Epochs, tr_loss, 'r', label= 'Training loss')\n",
    "    plt.plot(Epochs, val_loss, 'g', label= 'Validation loss')\n",
    "    plt.scatter(index_loss + 1, val_lowest, s= 150, c= 'blue', label= loss_label)\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(Epochs, tr_acc, 'r', label= 'Training Accuracy')\n",
    "    plt.plot(Epochs, val_acc, 'g', label= 'Validation Accuracy')\n",
    "    plt.scatter(index_acc + 1 , acc_highest, s= 150, c= 'blue', label= acc_label)\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout\n",
    "    plt.show()\n",
    "    \n",
    "makeGraph(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training : Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model.trainable = True\n",
    "for layer in pretrained_model.layers:\n",
    "    if isinstance(layer, layers.BatchNormalization): # 만약 layer == BatchNormalization layer이면, 학습층을 freezing합니다.\n",
    "        layer.trainable = False\n",
    "        \n",
    "# let`s see first 10 layers\n",
    "for l in pretrained_model.layers[:10]:\n",
    "    print(l.name, l.trainable)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(0.00001), # fine tuning을 할 때에는 learning_rate을 매우 작게 설정합니다.\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "# model.load_weights('./checkpoints/my_checkpoint')\n",
    "print(model.summary())\n",
    "history = model.fit(\n",
    "    train_images,\n",
    "    steps_per_epoch=len(train_images),\n",
    "    validation_data=val_images,\n",
    "    validation_steps=len(val_images),\n",
    "    epochs=3,\n",
    "    callbacks=[\n",
    "        EarlyStopping(monitor = \"val_loss\", # watch the val loss metric\n",
    "                               patience = 5,\n",
    "                               restore_best_weights = True), # if val loss decreases for 5 epochs in a row, stop training,\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, mode='min') \n",
    "    ]\n",
    ")\n",
    "model.save('./my_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "makeGraph(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model.evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(test_images, verbose=0)\n",
    "\n",
    "print(\"    Test Loss: {:.5f}\".format(results[0]))\n",
    "print(\"Test Accuracy: {:.2f}%\".format(results[1] * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F1 score / Recall / Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = test_images.classes\n",
    "y_pred = np.argmax(model.predict(test_images), axis = 1)\n",
    "f1 = f1_score(y_true, y_pred, average='macro')\n",
    "print(\"F1 Score:\", f1)\n",
    "print(classification_report(y_true, y_pred, target_names=test_images.class_indices.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = dict(zip(test_images.class_indices.values(), test_images.class_indices.keys()))\n",
    "Predictions = pd.DataFrame({\"Image Index\" : list(range(len(test_images.labels))), \n",
    "                            \"Test Labels\" : test_images.labels, \n",
    "                            \"Test Classes\" : [classes[i] for i in test_images.labels],\n",
    "                            \"Prediction Labels\" : y_pred,\n",
    "                            \"Prediction Classes\" : [classes[i] for i in y_pred],\n",
    "                            \"Path\": test_images.filenames,\n",
    "                            \"Prediction Probability\" : [x for x in np.asarray(tf.reduce_max(model.predict(test_images), axis = 1))]\n",
    "                           })\n",
    "Predictions.head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
